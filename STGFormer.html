<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="style_project_page.css" media="screen"/>
<link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<link rel="stylesheet" href="https://unpkg.com/@glidejs/glide/dist/css/glide.core.min.css">


<html lang="en">
<head>
	<title>STGFormer: Spatio-Temporal GraphFormer for 3D Human Pose Estimation in Video</title>
    <link rel="icon" type="image/png" href="./images/icon.jpg">

    <meta charset="UTF-8">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src=""></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <h1 class="project-title">
        STGFormer: Spatio-Temporal GraphFormer for 3D Human Pose Estimation in Video
    </h1>

    <!-- <div class="conference">
        In CVPR 2023
    </div> -->

    <!-- <br><br> -->


    <div class="authors">
        <a href=https://liuyangme.github.io>
            Yang Liu
        </a>
        <a href=https://sece.sysu.edu.cn/szll/js/kjdz/1135754.htm>
            Zhiyong Zhang
        </a>
        <!-- <a href=https://qq.github.io/>
            name <sup>3</sup>
        </a> -->
    </div>
    <br>

    <div class="affiliations">
        <span> Sun Yat-sen University</span></br>
        <!-- <span><sup>2</sup> Amazon Prime Video</span> </br> -->
    </div>
    <br><br>

    <div class="project-icons">
        <a href="https://arxiv.org/pdf/2407.10099">
            <i class="fa fa-file-pdf-o"></i> <br/>
            Paper
        </a>
        <a href="https://github.com/liuyangme/STGFormer">
            <i class="fa fa-github"></i> <br/>
            Code <br/>
        </a>
        <!-- <a href="https://www.youtube.com/watch?v=2xVNrGpGldM&t=5s">
            <i class="fa fa-youtube-play"></i> <br/>
            Video
        </a> -->
       <!--  <a href="https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/658/POP_poster.pdf">
            <i class="fa fa-newspaper-o"></i> <br/>
            Poster
        </a> -->
       <!--  <a href="https://pop.is.tue.mpg.de/">
            <i class="fa fa-database"></i> <br/>
            Dataset
        </a> -->

    </div>

    <div class="teaser">
        <br>
        <p style="width: 80%; text-align: justify;">
            We introduce <b>STGFormer</b>, a novel model that integrates long-range temporal-spatial dependencies with graph structural information in its attention mechanisms and utilizes a dual-path modulated hop-wise regular GCN Module to enhance the extraction of graph information for 3D human pose estimation in videos.
        </p>
        <br>
        <img src="./research/STGFormer/fig2_framework.jpg" alt="Teaser figure."/>
        <!-- <br> -->

    </div>

    <br><br>
    
    <hr>
    <h1>Abstract</h1>

    <p style="width: 85%">
        The current methods of video-based 3D human pose estimation have achieved significant progress; however, they continue to confront the significant challenge of depth ambiguity. To address this limitation, this paper presents the spatio-temporal GraphFormer framework for 3D human pose estimation in video, which integrates body structure graph-based representations with spatio-temporal information. Specifically, we develop a spatio-temporal criss-cross graph (STG) attention mechanism. This approach is designed to learn the long-range dependencies in data across both time and space, integrating graph information directly into the respective attention layers. Furthermore, we introduce the dual-path modulated hop-wise regular GCN (MHR-GCN) module, which utilizes modulation to optimize parameter usage and employs spatio-temporal hop-wise skip connections to acquire higher-order information. Additionally, this module processes temporal and spatial dimensions independently to learn their respective features while avoiding mutual influence. Finally, we demonstrate that our method achieves state-of-the-art performance in 3D human pose estimation on the Human3.6M and MPI-INF-3DHP datasets. </p>
    </p>

    <!-- <hr>
    <h1>Quantitative Results</h1>
    <img src="./research/PoseFormerV2/figures/quantitative_results.jpg" style="width: 85%" alt="PoseFormerV2 features."/></br>
    <p style="width: 85%">
        Our method outperforms PoseFormerV1 [1] (left) and other transformer-based methods (right) in terms of speed-accuracy trade-off. RF denotes Receptive Field and k×RF indicates that the ratio between the full sequence length and the number of frames as input into the spatial encoder of PoseFormerV2 is k, i.e., the RF of the spatial encoder is expanded by k× with a few low-frequency coefficients of the full sequence.</p></br>

    <h1>Qualitative Results</h1>
    <img src="./research/PoseFormerV2/figures/visualization.jpg" style="width: 85%" alt="PoseFormerV2 features."/></br>
    <p style="width: 85%">
        Qualitative results of PoseFormerV2 under challenging in-the-wild images: (a) Occlusions; (b)(c) Missed 2D joint detection; (d) Switched 2D joints. We highlight the unreliable 2D detection with light-yellow circles and corresponding 3D pose estimations with orange circles. PoseFormerV2 shows great robustness to imperfect 2D joint detection.</p></br>

    <img src="./research/PoseFormerV2/figures/noise_comparison.jpg" style="width: 85%" alt="PoseFormerV2 features."/>
    <p style="width: 85%">
        Qualitative comparisons of PoseFormerV2 with MHFormer [2] and PoseFormerV1 [1]. We randomly add Gaussian noise to the 2D detection of a specific joint. We highlight the deviated 2D detection with light-yellow arrows and corresponding 3D pose estimations with orange arrows. PoseFormerV2 shows better robustness to highly noisy input than existing methods.</p></br>

    <hr>
    <h1>Video</h1>


    <div class="video-container">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/2xVNrGpGldM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div></br>

    <p style="width: 85%">
        In the following, we provide the demo at the end of the introduction video. Here we add strong Gaussian noise to the detected 2D human pose and our method shows a suprisingly good <b>temporal consistency</b> under highly unreliable 2D detection.</p>
    <img src="./research/PoseFormerV2/figures/basketball.gif" style="width:85%;" height="40%"> -->
<!--     <div class="video-container" style="padding-top: 28%;">
        <iframe 
                src="https://media.githubusercontent.com/media/QitaoZhao/QitaoZhao.github.io/main/research/PoseFormerV2/figures/basketball.mp4" 
                frameBorder="0" 
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div> -->

    <!-- <p style="width: 90%; text-align: center;font-size: 14pt;">
    </br>
        中国大陆的朋友可<a href='https://www.bilibili.com/video/BV1KQ4y1z7Sc?share_source=copy_web'>在B站观看</a> | The video is also available on <a href='https://www.bilibili.com/video/BV1KQ4y1z7Sc?share_source=copy_web'>Bilibili</a> from mainland China
    </p> -->


    <br>

    <hr>
    <h1>Paper</h1>
       <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.pdf">
        <img src="research/STGFormer/paper_fig.jpg" style="width:85%;" height="auto"></a>

       <div class="paper-info">
       <br>
       <span style="font-size: 14pt; font-weight: bold;">STGFormer: Spatio-Temporal GraphFormer for 3D Human Pose Estimation in Video</span><br>
       <span style="font-size: 14pt;"> Yang Liu and Zhiyong Zhang. </span>  <br>
       <!-- <span style="font-size: 14pt;">In CVPR 2023</span>  -->
       <!-- <br> -->
   <!--     <span style="font-size: 14pt;">
        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_The_Power_of_Points_for_Modeling_Humans_in_Clothing_ICCV_2021_paper.pdf" target="_blank" rel="noopener">[Paper (CVF Version)]</a>&nbsp; <a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Ma_The_Power_of_ICCV_2021_supplemental.pdf" target="_blank" rel="noopener">[Supp]</a>&nbsp;
       <a href="https://arxiv.org/abs/2109.01137" target="_blank" rel="noopener">[arXiv]<br /></a> -->
    
       <pre><code>@article{liu2024stgformer,
    title={STGFormer: Spatio-Temporal GraphFormer for 3D Human Pose Estimation in Video},
    author={Liu, Yang and Zhang, Zhiyong},
    journal={arXiv preprint arXiv:2407.10099},
    year={2024}
}
</code></pre>
</div>

    <br><br>

    <hr>
    <!-- <h1>References</h1>
    <p style="width: 85%;text-align:left; ">
        [1] Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen, and Zhengming Ding. <b>3d human pose estimation with spatial and temporal transformers</b>. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11656–11665, October 2021.<br>

        [2] Wenhao Li, Hong Liu, Hao Tang, Pichao Wang, and Luc Van Gool. <b>Mhformer: Multi-hypothesis transformer for 3d human pose estimation</b>. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13147–13156, June 2022.<br><br> -->

    <!-- <b><a href="https://cape.is.tue.mpg.de/">Learning to Dress 3D People in Generative Clothing (CVPR 2020)</a></b> <br>
    <i>Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, Michael J. Black</i><br>
    CAPE &mdash; a generative model and a large-scale dataset for 3D clothed human meshes in varied poses and garment types. 
    We trained POP using the <a href="https://cape.is.tue.mpg.de/dataset">CAPE dataset</a>, check it out! -->

    <!-- </p> -->

    <!-- <br><br> -->
    <!-- <hr>
    <h1>Acknowledgements</h1>

    <p style="width: 85%;">
        The work was done while Qitao was a research intern mentored by Chen Chen.
        Qitao acknowledges the insightful advices from co-authors and CVPR'23 reviewers.
        The webpage template is adapted from 
        <a href="https://qianlim.github.io/POP">POP</a>.
    
    </p>

    <br><br> -->
</div>

</body>

</html>